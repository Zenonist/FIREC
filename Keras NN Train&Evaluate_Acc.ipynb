{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47cef9d",
   "metadata": {},
   "source": [
    "### Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40bbd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ASUS\\\\Desktop\\\\Github\\\\Face_Recognition_with_mask'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "import os \n",
    "os.getcwd()\n",
    "os.chdir('C:/Users/ASUS/Desktop/Github/Face_Recognition_with_mask')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5b4cc",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6691877f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90-100</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>Indian</th>\n",
       "      <th>Latino_Hispanic</th>\n",
       "      <th>Middle_Eastern</th>\n",
       "      <th>White</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001511</td>\n",
       "      <td>-0.007979</td>\n",
       "      <td>-0.021249</td>\n",
       "      <td>0.010664</td>\n",
       "      <td>-0.007335</td>\n",
       "      <td>-0.003255</td>\n",
       "      <td>0.012502</td>\n",
       "      <td>0.014529</td>\n",
       "      <td>-0.005273</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002029</td>\n",
       "      <td>-0.007151</td>\n",
       "      <td>-0.017678</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>-0.011775</td>\n",
       "      <td>-0.004272</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.021701</td>\n",
       "      <td>-0.015278</td>\n",
       "      <td>0.006131</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002845</td>\n",
       "      <td>-0.010357</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>-0.001559</td>\n",
       "      <td>-0.012745</td>\n",
       "      <td>0.014603</td>\n",
       "      <td>-0.004680</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002517</td>\n",
       "      <td>-0.008903</td>\n",
       "      <td>-0.010357</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>-0.001825</td>\n",
       "      <td>-0.004463</td>\n",
       "      <td>-0.000680</td>\n",
       "      <td>0.021333</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000587</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>-0.009970</td>\n",
       "      <td>0.007103</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>-0.011554</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>0.001518</td>\n",
       "      <td>-0.004695</td>\n",
       "      <td>-0.003033</td>\n",
       "      <td>0.022928</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>0.010925</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>0.040531</td>\n",
       "      <td>0.010064</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8292</th>\n",
       "      <td>0.031696</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>-0.016892</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>-0.003793</td>\n",
       "      <td>0.023630</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>-0.008832</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8293</th>\n",
       "      <td>0.018732</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>-0.009036</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>0.026952</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>-0.005313</td>\n",
       "      <td>0.026876</td>\n",
       "      <td>-0.002605</td>\n",
       "      <td>0.005254</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8294</th>\n",
       "      <td>0.004887</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>0.017752</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.027086</td>\n",
       "      <td>0.010898</td>\n",
       "      <td>0.016034</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8295</th>\n",
       "      <td>0.004260</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.008256</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>0.006344</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8296 rows × 2641 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.001511 -0.007979 -0.021249  0.010664 -0.007335 -0.003255  0.012502   \n",
       "1    -0.002029 -0.007151 -0.017678  0.007096 -0.011775 -0.004272  0.004986   \n",
       "2    -0.002845 -0.010357 -0.010958  0.013247  0.004672 -0.001559 -0.012745   \n",
       "3     0.002517 -0.008903 -0.010357  0.005166 -0.001825 -0.004463 -0.000680   \n",
       "4    -0.000587  0.001859 -0.009970  0.007103  0.002579 -0.000166 -0.000248   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8291  0.001518 -0.004695 -0.003033  0.022928  0.020972  0.010925 -0.001013   \n",
       "8292  0.031696 -0.001501 -0.016892  0.020605  0.025902  0.013530 -0.003793   \n",
       "8293  0.018732 -0.009901 -0.009036  0.025380  0.026952  0.013855 -0.005313   \n",
       "8294  0.004887  0.008089  0.010522  0.017752  0.018814  0.007535  0.009224   \n",
       "8295  0.004260 -0.000159  0.004356  0.004922  0.004986  0.005916  0.008256   \n",
       "\n",
       "             7         8         9  ...  90-100  Men  Women  Asian  Black  \\\n",
       "0     0.014529 -0.005273  0.009105  ...       0    0      1      0      0   \n",
       "1     0.021701 -0.015278  0.006131  ...       0    0      1      0      0   \n",
       "2     0.014603 -0.004680  0.005172  ...       0    1      0      0      0   \n",
       "3     0.021333  0.002563  0.005380  ...       0    0      1      0      0   \n",
       "4     0.008783 -0.011554  0.006251  ...       0    0      1      0      0   \n",
       "...        ...       ...       ...  ...     ...  ...    ...    ...    ...   \n",
       "8291  0.040531  0.010064  0.008347  ...       0    0      1      0      0   \n",
       "8292  0.023630  0.002694 -0.008832  ...       0    0      1      0      0   \n",
       "8293  0.026876 -0.002605  0.005254  ...       0    0      1      0      0   \n",
       "8294  0.027086  0.010898  0.016034  ...       0    0      1      0      0   \n",
       "8295  0.004874 -0.000095  0.006344  ...       0    1      0      0      0   \n",
       "\n",
       "      Indian  Latino_Hispanic  Middle_Eastern  White  id  \n",
       "0          0                0               1      0   0  \n",
       "1          0                1               0      0   0  \n",
       "2          0                0               0      1   0  \n",
       "3          0                0               0      1   0  \n",
       "4          0                0               0      1   0  \n",
       "...      ...              ...             ...    ...  ..  \n",
       "8291       0                0               0      1  49  \n",
       "8292       0                0               0      1  49  \n",
       "8293       0                0               0      1  49  \n",
       "8294       0                0               0      1  49  \n",
       "8295       0                0               0      1  49  \n",
       "\n",
       "[8296 rows x 2641 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_label_list = ['0-9','10-19','20-29','30-39','40-49','50-59','60-69','70-79','80-89','90-100']\n",
    "\n",
    "file = open(\"C:/Users/ASUS/Desktop/Github/Face_Recognition_with_mask/Data/Train_dataset_50id_vector_Non_masked.txt\",'r')\n",
    "lines = file.readlines()\n",
    "first = True\n",
    "df = None\n",
    "count = 1\n",
    "vector_list = []\n",
    "gender_list = []\n",
    "race_list = []\n",
    "age_list = []\n",
    "id_list = []\n",
    "for line in lines:\n",
    "    data = line.split('|')\n",
    "    image_id = line.split('/')[3]\n",
    "    image_id = image_id[:image_id.find('\\\\')]\n",
    "    id_list.append(image_id)\n",
    "    list_of_vector = data[1].split(' ')\n",
    "    list_of_vector = map(float, list_of_vector)\n",
    "    list_of_vector = list(list_of_vector)\n",
    "    vector_list.append(list_of_vector)\n",
    "    list_age = data[2].split(' ')\n",
    "    list_gender = data[3].split(' ')\n",
    "    list_race = data[4].split(' ') \n",
    "    if np.argmax(list_gender) == 0:\n",
    "        gender_list.append('Women')\n",
    "#         print(str(count) + 'A')\n",
    "    elif np.argmax(list_gender) == 1:\n",
    "        gender_list.append('Men')   \n",
    "    if np.argmax(list_race) == 0:\n",
    "        race_list.append('Asian')\n",
    "    elif np.argmax(list_race) == 1:\n",
    "        race_list.append('Indian')\n",
    "    elif np.argmax(list_race) == 2:\n",
    "        race_list.append('Black')\n",
    "    elif np.argmax(list_race) == 3:\n",
    "        race_list.append('White')\n",
    "    elif np.argmax(list_race) == 4:\n",
    "        race_list.append('Middle_Eastern')\n",
    "    elif np.argmax(list_race) == 5:\n",
    "        race_list.append('Latino_Hispanic') \n",
    "#     print(list_age)\n",
    "    list_age = map(float, list_age)\n",
    "    list_age = list(list_age)\n",
    "#     print(list_age)\n",
    "    age_predictions = np.array(list_age)\n",
    "#     print(age_predictions.dtype)\n",
    "    output_indexes = np.array([i for i in range(0, 101)])\n",
    "#     print(output_indexes.dtype)\n",
    "    apparent_age = np.sum(age_predictions * output_indexes)\n",
    "    apparent_age = round(apparent_age)\n",
    "    if apparent_age >= 0 and apparent_age <= 9:\n",
    "        age_list.append('0-9')\n",
    "#         print('A')\n",
    "    elif apparent_age >= 10 and apparent_age <= 19:\n",
    "        age_list.append('10-19')\n",
    "#         print('B')\n",
    "    elif apparent_age >= 20 and apparent_age <= 29:\n",
    "        age_list.append('20-29')\n",
    "#         print('C')\n",
    "    elif apparent_age >= 30 and apparent_age <= 39:\n",
    "        age_list.append('30-39')\n",
    "#         print('D')\n",
    "    elif apparent_age >= 40 and apparent_age <= 49:\n",
    "        age_list.append('40-49')\n",
    "#         print('E')\n",
    "    elif apparent_age >= 50 and apparent_age <= 59:\n",
    "        age_list.append('50-59')\n",
    "#         print('F')\n",
    "    elif apparent_age >= 60 and apparent_age <= 69:\n",
    "        age_list.append('60-69')\n",
    "#         print('G')\n",
    "    elif apparent_age >= 70 and apparent_age <= 79:\n",
    "        age_list.append('70-79')\n",
    "#         print('H')\n",
    "    elif apparent_age >= 80 and apparent_age <= 89:                   \n",
    "        age_list.append('80-89')\n",
    "#         print('I')\n",
    "    elif apparent_age >= 90 and apparent_age <= 100:\n",
    "        age_list.append('90-100')\n",
    "#         print('J')\n",
    "\n",
    "#     print(apparent_age)\n",
    "#     print(len(list_age))\n",
    "#     print(count)\n",
    "#     print('-=-=-=-=-=')\n",
    "#     count += 1\n",
    "#     vector = np.array(list_of_vector)\n",
    "#     vector.reshape(-1, 2622, 1)\n",
    "#     print(len(list_of_vector))\n",
    "    \n",
    "    \n",
    "#     print(np.argmax(list_gender,axis=0))\n",
    "\n",
    "# print(main_dict_list[0])\n",
    "# dataframe_masked_train = pd.DataFrame.from_dict(train_dict_list_masked)\n",
    "# print(len(gender_list))\n",
    "# print(len(race_list))\n",
    "# print(len(age_list))\n",
    "dataframe_train = pd.DataFrame(vector_list)\n",
    "one_hot_gender = pd.get_dummies(gender_list)\n",
    "one_hot_race = pd.get_dummies(race_list)\n",
    "one_hot_age = pd.get_dummies(age_list)\n",
    "\n",
    "for i in range(10):\n",
    "    if age_label_list[i] not in one_hot_age.columns:\n",
    "        one_hot_age.insert(loc=i,column=age_label_list[i],value = 0)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(id_list)\n",
    "encoded_id_train = le.transform(id_list)\n",
    "\n",
    "dataframe_train = dataframe_train.join(one_hot_age)\n",
    "dataframe_train = dataframe_train.join(one_hot_gender)\n",
    "dataframe_train = dataframe_train.join(one_hot_race)\n",
    "dataframe_train['id'] = encoded_id_train\n",
    "dataframe_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf1ebdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90-100</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>Indian</th>\n",
       "      <th>Latino_Hispanic</th>\n",
       "      <th>Middle_Eastern</th>\n",
       "      <th>White</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005811</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.006527</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000615</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.008273</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.004168</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.003481</td>\n",
       "      <td>0.017883</td>\n",
       "      <td>-0.002308</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.008259</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.014343</td>\n",
       "      <td>0.013360</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.017290</td>\n",
       "      <td>0.014120</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003455</td>\n",
       "      <td>-0.003399</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.010312</td>\n",
       "      <td>0.004979</td>\n",
       "      <td>0.024713</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>0.020256</td>\n",
       "      <td>0.018522</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>0.014161</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.017251</td>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.030316</td>\n",
       "      <td>-0.001384</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>-0.002087</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>0.010095</td>\n",
       "      <td>0.012177</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.036137</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.028041</td>\n",
       "      <td>0.012473</td>\n",
       "      <td>0.016539</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>0.014688</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.033331</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.019660</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.028804</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>0.018985</td>\n",
       "      <td>0.005582</td>\n",
       "      <td>0.009959</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.031971</td>\n",
       "      <td>0.009151</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.028676</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.012933</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>0.013410</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>0.022205</td>\n",
       "      <td>0.030659</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>-0.001834</td>\n",
       "      <td>0.018838</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.030777</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2087 rows × 2641 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.005811  0.006799  0.006527  0.006695  0.014906  0.011761  0.002441   \n",
       "1    -0.000615  0.004171  0.008273  0.014302  0.010149  0.014710  0.005147   \n",
       "2    -0.004168 -0.001410  0.005084  0.008728  0.003481  0.017883 -0.002308   \n",
       "3     0.000304  0.011040  0.014343  0.013360  0.009079  0.017290  0.014120   \n",
       "4    -0.003455 -0.003399  0.012119  0.010312  0.004979  0.024713  0.004397   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2082  0.014161  0.008186  0.017251  0.013830  0.030316 -0.001384  0.008802   \n",
       "2083  0.010095  0.012177  0.013166  0.010713  0.036137  0.000886  0.002403   \n",
       "2084  0.014688  0.009354  0.012795  0.011314  0.033331  0.011004  0.000947   \n",
       "2085  0.018985  0.005582  0.009959  0.016992  0.031971  0.009151 -0.000118   \n",
       "2086  0.013410  0.011762 -0.004382  0.022205  0.030659  0.004388 -0.001834   \n",
       "\n",
       "             7         8         9  ...  90-100  Men  Women  Asian  Black  \\\n",
       "0     0.014137  0.001551  0.000994  ...       0    1      0      0      0   \n",
       "1     0.013600  0.005837  0.008584  ...       0    0      1      0      0   \n",
       "2     0.017414  0.006853  0.008259  ...       0    0      1      0      0   \n",
       "3     0.020558 -0.001548  0.008252  ...       0    0      1      0      0   \n",
       "4     0.020256  0.018522  0.009038  ...       0    1      0      0      0   \n",
       "...        ...       ...       ...  ...     ...  ...    ...    ...    ...   \n",
       "2082  0.016736 -0.002087  0.004245  ...       0    0      1      0      0   \n",
       "2083  0.028041  0.012473  0.016539  ...       0    1      0      0      0   \n",
       "2084  0.019660  0.006582  0.028804  ...       0    0      1      0      0   \n",
       "2085  0.028676  0.006455  0.012933  ...       0    1      0      0      0   \n",
       "2086  0.018838  0.002721  0.030777  ...       0    1      0      0      0   \n",
       "\n",
       "      Indian  Latino_Hispanic  Middle_Eastern  White  id  \n",
       "0          0                0               0      1   0  \n",
       "1          0                0               0      1   0  \n",
       "2          0                0               0      1   0  \n",
       "3          0                0               0      1   0  \n",
       "4          0                0               0      1   0  \n",
       "...      ...              ...             ...    ...  ..  \n",
       "2082       0                0               0      1  49  \n",
       "2083       1                0               0      0  49  \n",
       "2084       0                0               0      1  49  \n",
       "2085       0                0               0      1  49  \n",
       "2086       0                0               0      1  49  \n",
       "\n",
       "[2087 rows x 2641 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C:/Users/ASUS/Desktop/Github/Face_Recognition_with_mask/Data/Test_dataset_50id_vector_masked.txt\"\n",
    "\n",
    "age_label_list = ['0-9','10-19','20-29','30-39','40-49','50-59','60-69','70-79','80-89','90-100']\n",
    "\n",
    "file = open(\"C:/Users/ASUS/Desktop/Github/Face_Recognition_with_mask/Data/Test_dataset_50id_vector_Masked_Ver_2.txt\",'r')\n",
    "lines = file.readlines()\n",
    "first = True\n",
    "df = None\n",
    "count = 1\n",
    "vector_list = []\n",
    "gender_list = []\n",
    "race_list = []\n",
    "age_list = []\n",
    "id_list = []\n",
    "for line in lines:\n",
    "    data = line.split('|')\n",
    "    image_id = line.split('/')[3]\n",
    "    image_id = image_id[:image_id.find('\\\\')]\n",
    "    id_list.append(image_id)\n",
    "    list_of_vector = data[1].split(' ')\n",
    "    list_of_vector = map(float, list_of_vector)\n",
    "    list_of_vector = list(list_of_vector)\n",
    "    vector_list.append(list_of_vector)\n",
    "    list_age = data[2].split(' ')\n",
    "    list_gender = data[3].split(' ')\n",
    "    list_race = data[4].split(' ') \n",
    "    if np.argmax(list_gender) == 0:\n",
    "        gender_list.append('Women')\n",
    "    elif np.argmax(list_gender) == 1:\n",
    "        gender_list.append('Men')   \n",
    "    if np.argmax(list_race) == 0:\n",
    "        race_list.append('Asian')\n",
    "    elif np.argmax(list_race) == 1:\n",
    "        race_list.append('Indian')\n",
    "    elif np.argmax(list_race) == 2:\n",
    "        race_list.append('Black')\n",
    "    elif np.argmax(list_race) == 3:\n",
    "        race_list.append('White')\n",
    "    elif np.argmax(list_race) == 4:\n",
    "        race_list.append('Middle_Eastern')\n",
    "    elif np.argmax(list_race) == 5:\n",
    "        race_list.append('Latino_Hispanic') \n",
    "    list_age = map(float, list_age)\n",
    "    list_age = list(list_age)\n",
    "    age_predictions = np.array(list_age)\n",
    "    output_indexes = np.array([i for i in range(0, 101)])\n",
    "    apparent_age = np.sum(age_predictions * output_indexes)\n",
    "    apparent_age = round(apparent_age)\n",
    "    if apparent_age >= 0 and apparent_age <= 9:\n",
    "        age_list.append('0-9')\n",
    "    elif apparent_age >= 10 and apparent_age <= 19:\n",
    "        age_list.append('10-19')\n",
    "    elif apparent_age >= 20 and apparent_age <= 29:\n",
    "        age_list.append('20-29')\n",
    "    elif apparent_age >= 30 and apparent_age <= 39:\n",
    "        age_list.append('30-39')\n",
    "    elif apparent_age >= 40 and apparent_age <= 49:\n",
    "        age_list.append('40-49')\n",
    "    elif apparent_age >= 50 and apparent_age <= 59:\n",
    "        age_list.append('50-59')\n",
    "    elif apparent_age >= 60 and apparent_age <= 69:\n",
    "        age_list.append('60-69')\n",
    "    elif apparent_age >= 70 and apparent_age <= 79:\n",
    "        age_list.append('70-79')\n",
    "    elif apparent_age >= 80 and apparent_age <= 89:                   \n",
    "        age_list.append('80-89')\n",
    "    elif apparent_age >= 90 and apparent_age <= 100:\n",
    "        age_list.append('90-100')\n",
    "\n",
    "dataframe_test = pd.DataFrame(vector_list)\n",
    "one_hot_gender = pd.get_dummies(gender_list)\n",
    "one_hot_race = pd.get_dummies(race_list)\n",
    "one_hot_age = pd.get_dummies(age_list)\n",
    "\n",
    "for i in range(10):\n",
    "    if age_label_list[i] not in one_hot_age.columns:\n",
    "        one_hot_age.insert(loc=i,column=age_label_list[i],value = 0)\n",
    "\n",
    "# print(id_list)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(id_list)\n",
    "encoded_id_test = le.transform(id_list)        \n",
    "\n",
    "dataframe_test = dataframe_test.join(one_hot_age)\n",
    "dataframe_test = dataframe_test.join(one_hot_gender)\n",
    "dataframe_test = dataframe_test.join(one_hot_race)\n",
    "dataframe_test['id'] = encoded_id_test\n",
    "dataframe_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76a0db",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7be5fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6636 train examples\n",
      "1660 validation examples\n"
     ]
    }
   ],
   "source": [
    "age_label_list = ['0-9','10-19','20-29','30-39','40-49','50-59','60-69','70-79','80-89','90-100']\n",
    "gender_label_list = ['Men','Women']\n",
    "race_label_list = ['Asian','Black','Indian','Latino_Hispanic','Middle_Eastern','White']\n",
    "\n",
    "all_features_label_list = age_label_list + gender_label_list + race_label_list\n",
    "age_gender_label = race_label_list\n",
    "age_race_label = gender_label_list\n",
    "gender_race_label = age_label_list\n",
    "age_label = gender_label_list + race_label_list\n",
    "gender_label = age_label_list + race_label_list\n",
    "race_label = age_label_list + gender_label_list\n",
    "\n",
    "all_feature_activate = False\n",
    "if all_feature_activate is not True:\n",
    "    dataframe_train = dataframe_train.drop(all_features_label_list, axis=1)\n",
    "    dataframe_test = dataframe_test.drop(all_features_label_list, axis=1)\n",
    "\n",
    "train, val = train_test_split(dataframe_train, test_size=0.2, random_state=17)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "# print(len(dataframe_masked_test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4c9b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2613</th>\n",
       "      <th>2614</th>\n",
       "      <th>2615</th>\n",
       "      <th>2616</th>\n",
       "      <th>2617</th>\n",
       "      <th>2618</th>\n",
       "      <th>2619</th>\n",
       "      <th>2620</th>\n",
       "      <th>2621</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>0.009531</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>0.027317</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>-0.005734</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>-0.002560</td>\n",
       "      <td>0.022687</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003761</td>\n",
       "      <td>-0.009793</td>\n",
       "      <td>-0.003024</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>-0.015434</td>\n",
       "      <td>-0.010990</td>\n",
       "      <td>-0.003005</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5155</th>\n",
       "      <td>0.035676</td>\n",
       "      <td>-0.003842</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.007077</td>\n",
       "      <td>0.011967</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.032977</td>\n",
       "      <td>0.015107</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019023</td>\n",
       "      <td>-0.027523</td>\n",
       "      <td>-0.022306</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>-0.001775</td>\n",
       "      <td>-0.009504</td>\n",
       "      <td>-0.013008</td>\n",
       "      <td>-0.010018</td>\n",
       "      <td>-0.001794</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7053</th>\n",
       "      <td>0.014986</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>-0.012233</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.005923</td>\n",
       "      <td>0.009551</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007736</td>\n",
       "      <td>-0.010128</td>\n",
       "      <td>-0.012123</td>\n",
       "      <td>-0.001020</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.009618</td>\n",
       "      <td>-0.003140</td>\n",
       "      <td>-0.004004</td>\n",
       "      <td>-0.001260</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6591</th>\n",
       "      <td>0.014594</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>0.019973</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.009370</td>\n",
       "      <td>0.020481</td>\n",
       "      <td>-0.002007</td>\n",
       "      <td>-0.008076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>-0.010725</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.019741</td>\n",
       "      <td>-0.009193</td>\n",
       "      <td>-0.016483</td>\n",
       "      <td>-0.017217</td>\n",
       "      <td>-0.005673</td>\n",
       "      <td>0.008655</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>-0.002866</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.005438</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>0.010223</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>-0.001732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001233</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>-0.004005</td>\n",
       "      <td>-0.004954</td>\n",
       "      <td>-0.008047</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.015968</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2623 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "6626  0.009531 -0.000147  0.027317  0.002584 -0.005734  0.002536 -0.002560   \n",
       "5155  0.035676 -0.003842  0.001944  0.007035  0.007077  0.011967  0.000635   \n",
       "7053  0.014986 -0.004960 -0.012233  0.021403  0.005923  0.009551  0.001746   \n",
       "6591  0.014594 -0.002820  0.019973 -0.004267  0.001499  0.000641  0.009370   \n",
       "1158  0.005785  0.005078  0.000820 -0.002866  0.001802  0.005438  0.006097   \n",
       "\n",
       "             7         8         9  ...      2613      2614      2615  \\\n",
       "6626  0.022687  0.001713  0.007170  ... -0.003761 -0.009793 -0.003024   \n",
       "5155  0.032977  0.015107  0.003923  ... -0.019023 -0.027523 -0.022306   \n",
       "7053  0.013029  0.001418  0.009171  ...  0.007736 -0.010128 -0.012123   \n",
       "6591  0.020481 -0.002007 -0.008076  ...  0.008998 -0.010725  0.001120   \n",
       "1158  0.010223  0.009175 -0.001732  ... -0.001233  0.004778  0.006424   \n",
       "\n",
       "          2616      2617      2618      2619      2620      2621  id  \n",
       "6626 -0.001855  0.000413 -0.015434 -0.010990 -0.003005  0.004059  39  \n",
       "5155  0.002054 -0.001775 -0.009504 -0.013008 -0.010018 -0.001794  31  \n",
       "7053 -0.001020 -0.000024 -0.009618 -0.003140 -0.004004 -0.001260  42  \n",
       "6591  0.019741 -0.009193 -0.016483 -0.017217 -0.005673  0.008655  39  \n",
       "1158  0.004949 -0.004005 -0.004954 -0.008047  0.008832  0.015968   7  \n",
       "\n",
       "[5 rows x 2623 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80dbb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('id')\n",
    "#     labels = to_categorical(labels)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dataframe, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51d8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train)\n",
    "test_ds = df_to_dataset(dataframe_test, shuffle=False)\n",
    "val_ds = df_to_dataset(val, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4261e2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "A batch of targets: tf.Tensor(\n",
      "[ 4 19 25 40 49 20  1 38 41 19 17 37 42 14 44  6  8 24 28 42  9  2 13 41\n",
      " 43 34 42  2 35 38 37 49], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "for feature_batch, label_batch in val_ds.take(1):\n",
    "#     print(feature_batch)\n",
    "#     print('Every feature:', list(feature_batch.keys()))\n",
    "#     print('A batch of ages:', feature_batch['age'])\n",
    "    print('A batch of targets:', label_batch )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19907f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9dcf4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 ^ N\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(2622,))) #2622 for non-features 2640 for all features 2634 for age and gender 2638 for age and race 2630 for gender and race\n",
    "# model.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2048, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(50, activation='softmax')) #layer_number = id\n",
    "#Add layer to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9298231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 2048)              5371904   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                6450      \n",
      "=================================================================\n",
      "Total params: 7,771,826\n",
      "Trainable params: 7,771,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf4118b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44181e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\env_dlib\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4929: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 2s 4ms/step - loss: 1.5685 - accuracy: 0.5868 - val_loss: 0.5622 - val_accuracy: 0.8494\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.4128 - accuracy: 0.8858 - val_loss: 0.3471 - val_accuracy: 0.9024\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.2700 - accuracy: 0.9234 - val_loss: 0.3356 - val_accuracy: 0.9096\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.1985 - accuracy: 0.9448 - val_loss: 0.3352 - val_accuracy: 0.9084\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.1606 - accuracy: 0.9543 - val_loss: 0.3447 - val_accuracy: 0.9060\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.1258 - accuracy: 0.9620 - val_loss: 0.2874 - val_accuracy: 0.9223\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 1s 5ms/step - loss: 0.0989 - accuracy: 0.9708 - val_loss: 0.2859 - val_accuracy: 0.9301\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.1115 - accuracy: 0.9673 - val_loss: 0.3044 - val_accuracy: 0.9319\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0952 - accuracy: 0.9703 - val_loss: 0.2601 - val_accuracy: 0.9331\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0572 - accuracy: 0.9812 - val_loss: 0.3009 - val_accuracy: 0.9289\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0450 - accuracy: 0.9878 - val_loss: 0.2950 - val_accuracy: 0.9422\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0607 - accuracy: 0.9822 - val_loss: 0.2993 - val_accuracy: 0.9343\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0673 - accuracy: 0.9804 - val_loss: 0.2845 - val_accuracy: 0.9343\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0412 - accuracy: 0.9872 - val_loss: 0.2973 - val_accuracy: 0.9398\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0683 - accuracy: 0.9791 - val_loss: 0.3559 - val_accuracy: 0.9301\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0704 - accuracy: 0.9803 - val_loss: 0.3349 - val_accuracy: 0.9373\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0342 - accuracy: 0.9913 - val_loss: 0.3683 - val_accuracy: 0.9343\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0569 - accuracy: 0.9861 - val_loss: 0.3426 - val_accuracy: 0.9295\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0278 - accuracy: 0.9925 - val_loss: 0.2823 - val_accuracy: 0.9494\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.2913 - val_accuracy: 0.9500\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.2892 - val_accuracy: 0.9530\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0578 - accuracy: 0.9845 - val_loss: 0.4213 - val_accuracy: 0.9247\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0587 - accuracy: 0.9822 - val_loss: 0.3466 - val_accuracy: 0.9440\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0497 - accuracy: 0.9875 - val_loss: 0.3781 - val_accuracy: 0.9343\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 1s 5ms/step - loss: 0.0230 - accuracy: 0.9940 - val_loss: 0.2968 - val_accuracy: 0.9488\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0297 - accuracy: 0.9922 - val_loss: 0.3157 - val_accuracy: 0.9428\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0265 - accuracy: 0.9940 - val_loss: 0.3212 - val_accuracy: 0.9440\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 1s 5ms/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 0.3437 - val_accuracy: 0.9416\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.3087 - val_accuracy: 0.9530\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.3259 - val_accuracy: 0.9548\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.6299e-04 - accuracy: 0.9997 - val_loss: 0.3380 - val_accuracy: 0.9548\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 0.3515 - val_accuracy: 0.9548\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.5978e-04 - accuracy: 0.9997 - val_loss: 0.3582 - val_accuracy: 0.9554\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.9667e-04 - accuracy: 0.9995 - val_loss: 0.3700 - val_accuracy: 0.9554\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.7788e-04 - accuracy: 0.9995 - val_loss: 0.3854 - val_accuracy: 0.9554\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.4873e-04 - accuracy: 0.9995 - val_loss: 0.3958 - val_accuracy: 0.9548\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.7039e-04 - accuracy: 0.9995 - val_loss: 0.3996 - val_accuracy: 0.9548\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.1843e-04 - accuracy: 0.9995 - val_loss: 0.4118 - val_accuracy: 0.9548\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.5364e-04 - accuracy: 0.9995 - val_loss: 0.4163 - val_accuracy: 0.9548\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.9796e-04 - accuracy: 0.9995 - val_loss: 0.4255 - val_accuracy: 0.9542\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.0586e-04 - accuracy: 0.9997 - val_loss: 0.4289 - val_accuracy: 0.9542\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.9048e-04 - accuracy: 0.9995 - val_loss: 0.4405 - val_accuracy: 0.9524\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.0898e-04 - accuracy: 0.9997 - val_loss: 0.4476 - val_accuracy: 0.9530\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.2982e-04 - accuracy: 0.9995 - val_loss: 0.4706 - val_accuracy: 0.9536\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.3691e-04 - accuracy: 0.9997 - val_loss: 0.4828 - val_accuracy: 0.9518\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.9743e-04 - accuracy: 0.9995 - val_loss: 0.4874 - val_accuracy: 0.9530\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.9493e-04 - accuracy: 0.9995 - val_loss: 0.4819 - val_accuracy: 0.9524\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.3739e-04 - accuracy: 0.9995 - val_loss: 0.5066 - val_accuracy: 0.9512\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 8.3153e-04 - accuracy: 0.9997 - val_loss: 0.5153 - val_accuracy: 0.9518\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.1350 - accuracy: 0.9718 - val_loss: 0.6901 - val_accuracy: 0.8620\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.1517 - accuracy: 0.9617 - val_loss: 0.3073 - val_accuracy: 0.9416\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0340 - accuracy: 0.9911 - val_loss: 0.3261 - val_accuracy: 0.9404\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0163 - accuracy: 0.9953 - val_loss: 0.2962 - val_accuracy: 0.9530\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0306 - accuracy: 0.9917 - val_loss: 0.3257 - val_accuracy: 0.9404\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0234 - accuracy: 0.9944 - val_loss: 0.3030 - val_accuracy: 0.9512\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.4151 - val_accuracy: 0.9404\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0597 - accuracy: 0.9849 - val_loss: 0.3156 - val_accuracy: 0.9488\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0137 - accuracy: 0.9967 - val_loss: 0.3366 - val_accuracy: 0.9446\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0429 - accuracy: 0.9913 - val_loss: 0.3260 - val_accuracy: 0.9428\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0318 - accuracy: 0.9922 - val_loss: 0.3237 - val_accuracy: 0.9482\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0246 - accuracy: 0.9946 - val_loss: 0.3172 - val_accuracy: 0.9494\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0059 - accuracy: 0.9989 - val_loss: 0.2752 - val_accuracy: 0.9560\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.3025 - val_accuracy: 0.9566\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.7538e-04 - accuracy: 0.9995 - val_loss: 0.3129 - val_accuracy: 0.9590\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.5121e-04 - accuracy: 0.9997 - val_loss: 0.3268 - val_accuracy: 0.9584\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.9192e-04 - accuracy: 0.9995 - val_loss: 0.3394 - val_accuracy: 0.9590\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.1157e-04 - accuracy: 0.9995 - val_loss: 0.3535 - val_accuracy: 0.9584\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.5547e-04 - accuracy: 0.9997 - val_loss: 0.3758 - val_accuracy: 0.9590\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.7180e-04 - accuracy: 0.9995 - val_loss: 0.3964 - val_accuracy: 0.9590\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.6647e-04 - accuracy: 0.9995 - val_loss: 0.4183 - val_accuracy: 0.9578\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.8170e-04 - accuracy: 0.9995 - val_loss: 0.4366 - val_accuracy: 0.9578\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.9395e-04 - accuracy: 0.9997 - val_loss: 0.4531 - val_accuracy: 0.9578\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.1146e-04 - accuracy: 0.9995 - val_loss: 0.4747 - val_accuracy: 0.9578\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.1764e-04 - accuracy: 0.9995 - val_loss: 0.4784 - val_accuracy: 0.9578\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.5681e-04 - accuracy: 0.9995 - val_loss: 0.4913 - val_accuracy: 0.9578\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.8777e-04 - accuracy: 0.9995 - val_loss: 0.5059 - val_accuracy: 0.9572\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.8083e-04 - accuracy: 0.9995 - val_loss: 0.5239 - val_accuracy: 0.9578\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.7824e-04 - accuracy: 0.9997 - val_loss: 0.5092 - val_accuracy: 0.9578\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.8687e-04 - accuracy: 0.9995 - val_loss: 0.5270 - val_accuracy: 0.9578\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.1654e-04 - accuracy: 0.9997 - val_loss: 0.5374 - val_accuracy: 0.9572\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.7206e-04 - accuracy: 0.9995 - val_loss: 0.5454 - val_accuracy: 0.9566\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.9511e-04 - accuracy: 0.9997 - val_loss: 0.5576 - val_accuracy: 0.9566\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.7160e-04 - accuracy: 0.9997 - val_loss: 0.5572 - val_accuracy: 0.9572\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.0719e-04 - accuracy: 0.9997 - val_loss: 0.5760 - val_accuracy: 0.9572\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.5197e-04 - accuracy: 0.9997 - val_loss: 0.5805 - val_accuracy: 0.9566\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.8983e-04 - accuracy: 0.9997 - val_loss: 0.5909 - val_accuracy: 0.9566\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.5382e-04 - accuracy: 0.9997 - val_loss: 0.6007 - val_accuracy: 0.9572\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.4229e-04 - accuracy: 0.9997 - val_loss: 0.6095 - val_accuracy: 0.9560\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.4719e-04 - accuracy: 0.9997 - val_loss: 0.6181 - val_accuracy: 0.9560\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 7.2054e-04 - accuracy: 0.9995 - val_loss: 0.5781 - val_accuracy: 0.9542\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.7513e-04 - accuracy: 0.9997 - val_loss: 0.6008 - val_accuracy: 0.9548\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.6890e-04 - accuracy: 0.9995 - val_loss: 0.6089 - val_accuracy: 0.9554\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.4488e-04 - accuracy: 0.9995 - val_loss: 0.6228 - val_accuracy: 0.9554\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.4501e-04 - accuracy: 0.9995 - val_loss: 0.6327 - val_accuracy: 0.9554\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.2636e-04 - accuracy: 0.9995 - val_loss: 0.6412 - val_accuracy: 0.9554\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.4086e-04 - accuracy: 0.9995 - val_loss: 0.6412 - val_accuracy: 0.9548\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.5133e-04 - accuracy: 0.9995 - val_loss: 0.6602 - val_accuracy: 0.9536\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.3351e-04 - accuracy: 0.9997 - val_loss: 0.6493 - val_accuracy: 0.9530\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 6.3091e-04 - accuracy: 0.9995 - val_loss: 0.6666 - val_accuracy: 0.9530\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 1s 4ms/step - loss: 5.8683e-04 - accuracy: 0.9995 - val_loss: 0.6725 - val_accuracy: 0.9530\n",
      "It takes 90.81977653503418  seconds\n"
     ]
    }
   ],
   "source": [
    "TIC = time.time()\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100\n",
    ")\n",
    "TOC = time.time()\n",
    "print(\"It takes\",TOC-TIC,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "196999c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 7.6069 - accuracy: 0.5625\n",
      "Loss:  7.6068501472473145\n",
      "Accuracy:  0.5625299215316772\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548547dd",
   "metadata": {},
   "source": [
    "### Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43943cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(2632,)))\n",
    "    for i in range(hp.Int('layers', 1, 3)):\n",
    "        model.add(layers.Dense(units=hp.Choice('units_' + str(i), values=[128,256,512,1024,2048]), activation='relu'))    \n",
    "    model.add(tf.keras.layers.Dense(50, activation='softmax'))\n",
    "    model.compile('adam', 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50dc23ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project D:/Tuning/Keras\\kt_mixed_with_age\\oracle.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective = 'val_accuracy',\n",
    "    max_trials=100,\n",
    "    project_name='kt_mixed_with_race',\n",
    "    directory = 'D:/Tuning/Keras'   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a59011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf94bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': None}\n",
      "units_0 (Choice)\n",
      "{'default': 128, 'conditions': [], 'values': [128, 256, 512, 1024, 2048], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6fc851a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 02m 54s]\n",
      "val_accuracy: 0.918968141078949\n",
      "\n",
      "Best val_accuracy So Far: 0.9262518882751465\n",
      "Total elapsed time: 02h 41m 37s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_ds, epochs=100, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0f5e00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 73\n"
     ]
    }
   ],
   "source": [
    "# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# model = tuner.hypermodel.build(best_hps)\n",
    "# history = model.fit(train_ds,validation_data=val_ds,epochs=100)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b27ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "history = hypermodel.fit(train_ds,validation_data=val_ds,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b843e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 2632)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2048)              5392384   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                102450    \n",
      "=================================================================\n",
      "Total params: 9,691,186\n",
      "Trainable params: 9,691,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hypermodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cdb25780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 1.8376 - accuracy: 0.8548\n",
      "Loss:  1.837570309638977\n",
      "Accuracy:  0.8548155426979065\n",
      "Best epoch: 72\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = hypermodel.evaluate(test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a403003",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ef9adb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 2ms/step - loss: 1.2691 - accuracy: 0.8958\n",
      "Loss:  1.2690939903259277\n",
      "Accuracy:  0.8958333134651184\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "707f4f34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Non_masked_classification_For_Masked\\assets\n"
     ]
    }
   ],
   "source": [
    "# model.save('Non_masked_classification_For_Masked')\n",
    "# reloaded_model = tf.keras.models.load_model('masked_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d30fd66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 1.1233 - accuracy: 0.8721\n",
      "Loss:  1.123286247253418\n",
      "Accuracy:  0.8720651865005493\n"
     ]
    }
   ],
   "source": [
    "# reloaded_model = tf.keras.models.load_model('Hyper_mixed_classification_with_age')\n",
    "# loss, accuracy = reloaded_model.evaluate(test_ds)\n",
    "# print(\"Loss: \", loss)\n",
    "# print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196f536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
